{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWynlk/M+3Oh5bNQ74l2rL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarabdar/keras/blob/main/Custom_Training_loop_in_TensorFlow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p67L_mCzBVXE",
        "outputId": "01127c71-98f4-4a48-9091-337c82c06fdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import warnings\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Flatten, Input\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "import numpy as np\n",
        "\n",
        "# Suppress all Python warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set TensorFlow log level to suppress warnings and info messages\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
        "\n",
        "# Step 1: Set Up the Environment\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Define the Model\n",
        "\n",
        "model = Sequential([\n",
        "    Flatten(input_shape=(28, 28)),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "dL5amYNlBeNj"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Define Loss Function and Optimizer\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "optimizer = tf.keras.optimizers.Adam()\n",
        "\n",
        "\n",
        "accuracy_metric = tf.keras.metrics.SparseCategoricalAccuracy()  # Metric to track accuracy during training\n"
      ],
      "metadata": {
        "id": "J1lZ7QiEBhMr"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: Implement the Custom Training Loop\n",
        "\n",
        "epochs = 2\n",
        "# train_dataset = train_dataset.repeat(epochs)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
        "for epoch in range(epochs):\n",
        "    print(f'Start of epoch {epoch + 1}')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:  # GradientTape is use to record every calculation perform.\n",
        "            logits = model(x_batch_train, training=True)  # Forward pass\n",
        "            loss_value = loss_fn(y_batch_train, logits)  # Compute loss\n",
        "\n",
        "\n",
        "        # Now that we have record of calculations (model outputs and loss)\n",
        "        # We can compute gradients and update weights\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update the accuracy metric\n",
        "        accuracy_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log the loss and accuracy every 200 steps\n",
        "        if step % 200 == 0:\n",
        "            print(f'Epoch {epoch + 1} Step {step}: Loss = {loss_value.numpy()} Accuracy = {accuracy_metric.result().numpy()}')\n",
        "\n",
        "    # Reset the metric at the end of each epoch\n",
        "    accuracy_metric.reset_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYfT5r4-Bk9P",
        "outputId": "3127063a-5e7e-4ad6-a0c6-86a6694f265a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start of epoch 1\n",
            "Epoch 1 Step 0: Loss = 0.120081327855587 Accuracy = 0.96875\n",
            "Epoch 1 Step 200: Loss = 0.1890629678964615 Accuracy = 0.9480721354484558\n",
            "Epoch 1 Step 400: Loss = 0.11970341205596924 Accuracy = 0.9456047415733337\n",
            "Epoch 1 Step 600: Loss = 0.07614672183990479 Accuracy = 0.9485753178596497\n",
            "Epoch 1 Step 800: Loss = 0.09109409898519516 Accuracy = 0.9504915475845337\n",
            "Epoch 1 Step 1000: Loss = 0.39542412757873535 Accuracy = 0.9507992267608643\n",
            "Epoch 1 Step 1200: Loss = 0.1787436455488205 Accuracy = 0.951654851436615\n",
            "Epoch 1 Step 1400: Loss = 0.17438732087612152 Accuracy = 0.9522216320037842\n",
            "Epoch 1 Step 1600: Loss = 0.17052042484283447 Accuracy = 0.9521978497505188\n",
            "Epoch 1 Step 1800: Loss = 0.16195276379585266 Accuracy = 0.9534980654716492\n",
            "Start of epoch 2\n",
            "Epoch 2 Step 0: Loss = 0.04762498289346695 Accuracy = 1.0\n",
            "Epoch 2 Step 200: Loss = 0.07808659225702286 Accuracy = 0.9710820913314819\n",
            "Epoch 2 Step 400: Loss = 0.08706273883581161 Accuracy = 0.9685162305831909\n",
            "Epoch 2 Step 600: Loss = 0.02191304787993431 Accuracy = 0.9702578783035278\n",
            "Epoch 2 Step 800: Loss = 0.04702022671699524 Accuracy = 0.9707787036895752\n",
            "Epoch 2 Step 1000: Loss = 0.20444324612617493 Accuracy = 0.9714660048484802\n",
            "Epoch 2 Step 1200: Loss = 0.11883724480867386 Accuracy = 0.9718203544616699\n",
            "Epoch 2 Step 1400: Loss = 0.11405331641435623 Accuracy = 0.9719619750976562\n",
            "Epoch 2 Step 1600: Loss = 0.10726504027843475 Accuracy = 0.9715216755867004\n",
            "Epoch 2 Step 1800: Loss = 0.09153413772583008 Accuracy = 0.9717691540718079\n"
          ]
        }
      ]
    }
  ]
}